const tf = require('@tensorflow/tfjs');
const normalize = require('array-normalize')
 
var model = tf.sequential();
maxList = [];
minList = [];

const learningRate = 0.4;
const epochs = 1500;

// // DataSet de prueba
// testDS = [
//   [20.657308, -103.325083, 9],
//   [20.724842, 103.418746, 11],
//   [22.724842, 101.418746, 11],
//   [20.657252, -103.327146, 9],
//   [20.657252, -103.327146, 10],
//   [20.657252, -103.327146, 11],
//   [20.657252, -103.327146, 12],
//   [20.661904, -103.307510, 21]
// ];

// predict(model, testDS);

trainingSet = buildTrainingDS();
neuralNetwork(trainingSet, learningRate, epochs).then(()=>{
  console.log("Model training completed");
});


// Construcción y entrenamiento de la red neuronal
/**
 * 
 * @param {*} trainingSet 
 * @param {*} learningRate 
 * @param {*} epochs 
 * @returns 
 */
async function neuralNetwork(trainingSet, learningRate, epochs){
  const opt = tf.train.sgd(learningRate); // Gradiante descendiente como optimizador
  
  model.add(tf.layers.dense({  // Primera capa oculta
    units: 4, 
    inputShape: [3], 
    activation: 'sigmoid' 
  }));
  
  model.add(tf.layers.dense({ // Segunda capa oculta
    units: 4, 
    activation: 'sigmoid' 
  }));
  
  model.add(tf.layers.dense({ // Capa de salida
    units: 1, 
    activation: 'sigmoid' 
  }));
  
  // Compila el modelo
  // Utiliza la función de perdida logarítmica
  model.compile({optimizer: opt, loss: tf.losses.logLoss});
  
  // Entrenamiento del modelo
  const res = await model.fit(trainingSet.x, trainingSet.y, {
      epochs: epochs,
      callbacks: {
        onEpochEnd: (epoch, log) => {
          // console.log(`Epoch ${epoch}: loss = ${log.loss}`);
        },
      },
  });

  return model;
}


/**
 * Predice la probabilidad de incidencia
 * @param {*} model 
 */
function predict(data){
  var predictions = [];

  data.forEach((dp) => {
    dpn = []; // Data Point normalizado
  
    // Normalización de los valores de entrada
    for (let i = 0; i < dp.length; i++) {
      dpn.push(normalize([dp[i], minList[i], maxList[i]])[0]);
    }
    console.log(dp);
    console.log(dpn);
    
    const pr = model.predict(tf.tensor([dpn]));
    predictions.push(pr.dataSync()[0]);
  });

  return predictions;
}

/**
 * Construye un dataset de entrenamiento
 * @returns 
 */
function buildTrainingDS(){
  // Dataset de entrenamiento
  var Xunproc = [
    [20.657392, -103.320525, 13],
    [20.656176, -103.318919, 22],
    [20.653622, -103.322102, 20],
    [20.652978, -103.326746, 1],
    [20.654537, -103.322562, 6],
    [20.655842, -103.329214, 5],
    [20.651485, -103.327390, 23],
    [20.648433, -103.318035, 1],
    [20.650608, -103.333500, 22],
    [20.657139, -103.330314, 21],
    [20.665405, -103.327542, 19],
    [20.664903, -103.317462, 14],
    [20.663734, -103.333717, 11],
    [20.648476, -103.340122, 8],
    [20.640763, -103.316353, 20],
    [20.650396, -103.309495, 21],
    [20.645296, -103.324022, 9],
    [20.651902, -103.329086, 10],
    [20.656982, -103.331790, 22],
    [20.659854, -103.333871, 21],
    [20.667408, -103.329509, 15],
    [20.666605, -103.326076, 12],
    [20.664916, -103.321421, 19],
    
    [20.657392, -103.320525, 11],
    [20.656176, -103.318919, 21],
    [20.653622, -103.322102, 15],
    [20.652978, -103.326746, 10],
    [20.654537, -103.322562, 16],
    [20.655842, -103.329214, 5],
    [20.651485, -103.327390, 23],
    [20.648433, -103.318035, 1],
    [20.650608, -103.333500, 22],
    [20.657139, -103.330314, 21],
    [20.665405, -103.327542, 20],
    [20.664903, -103.317462, 11],
    [20.663734, -103.333717, 10],
    [20.648476, -103.340122, 15],
    [20.640763, -103.316353, 20],
    [20.650396, -103.309495, 21],
    [20.645296, -103.324022, 9],
    [20.651902, -103.329086, 10],
    [20.656982, -103.331790, 22],
    [20.659854, -103.333871, 19],
    [20.667408, -103.329509, 15],
    [20.666605, -103.326076, 10],
    [20.664916, -103.321421, 5],
  
    [20.657392, -103.320525, 20],
    [20.656176, -103.318919, 5],
    [20.653622, -103.322102, 5],
    [20.652978, -103.326746, 5],
    [20.654537, -103.322562, 8],
    [20.655842, -103.329214, 21],
    [20.651485, -103.327390, 22],
    [20.648433, -103.318035, 7],
    [20.650608, -103.333500, 22],
    [20.657139, -103.330314, 20],
    [20.665405, -103.327542, 10],
    [20.664903, -103.317462, 11],
    [20.663734, -103.333717, 11],
    [20.648476, -103.340122, 8],
    [20.640763, -103.316353, 9],
    [20.650396, -103.309495, 1],
    [20.645296, -103.324022, 6],
    [20.651902, -103.329086, 8],
    [20.656982, -103.331790, 5],
    [20.659854, -103.333871, 4],
    [20.667408, -103.329509, 6],
    [20.666605, -103.326076, 8],
    [20.664916, -103.321421, 9],
  
  
    [20.651755, -103.300547, 13],
    [20.673622, -103.334500, 22],
    [20.690855, -103.334134, 20],
    [20.673342, -103.388429, 1],
    [20.666456, -103.370151, 6],
    [20.670913, -103.389610, 5],
    [20.674382, -103.368960, 23],
    [20.677478, -103.354927, 1],
    [20.688996, -103.349895, 22],
    [20.693208, -103.374917, 21],
    [20.697417, -103.330305, 19],
    [20.672269, -103.296287, 14],
    [20.640405, -103.410248, 11],
    [20.673686, -103.430275, 8],
    [20.636733, -103.396480, 20],
    [20.622111, -103.363258, 21],
    [20.650782, -103.323830, 9],
    [20.657485, -103.324695, 10],
    [20.659695, -103.330599, 22],
    [20.647100, -103.320288, 21],
    [20.659694, -103.345712, 15],
    [20.667423, -103.267867, 12],
    [20.725243, -103.360712, 19],
  
    [20.651755, -103.300547, 20],
    [20.673622, -103.334500, 21],
    [20.690855, -103.334134, 18],
    [20.673342, -103.388429, 11],
    [20.666456, -103.370151, 9],
    [20.670913, -103.389610, 10],
    [20.674382, -103.368960, 17],
    [20.677478, -103.354927, 22],
    [20.688996, -103.349895, 23],
    [20.693208, -103.374917, 0],
    [20.697417, -103.330305, 5],
    [20.672269, -103.296287, 2],
    [20.640405, -103.410248, 7],
    [20.673686, -103.430275, 15],
    [20.636733, -103.396480, 14],
    [20.622111, -103.363258, 23],
    [20.650782, -103.323830, 15],
    [20.657485, -103.324695, 12],
    [20.659695, -103.330599, 13],
    [20.647100, -103.320288, 16],
    [20.659694, -103.345712, 13],
    [20.667423, -103.267867, 12],
    [20.725243, -103.360712, 12],
    
    [20.651755, -103.300547, 2],
    [20.673622, -103.334500, 4],
    [20.690855, -103.334134, 5],
    [20.673342, -103.388429, 1],
    [20.666456, -103.370151, 15],
    [20.670913, -103.389610, 17],
    [20.674382, -103.368960, 14],
    [20.677478, -103.354927, 11],
    [20.688996, -103.349895, 8],
    [20.693208, -103.374917, 10],
    [20.697417, -103.330305, 19],
    [20.672269, -103.296287, 12],
    [20.640405, -103.410248, 17],
    [20.673686, -103.430275, 22],
    [20.636733, -103.396480, 23],
    [20.622111, -103.363258, 21],
    [20.650782, -103.323830, 13],
    [20.657485, -103.324695, 16],
    [20.659695, -103.330599, 18],
    [20.647100, -103.320288, 20],
    [20.659694, -103.345712, 18],
    [20.667423, -103.267867, 17],
    [20.725243, -103.360712, 21],
  ];
  
  // Preprocesado de las entradas del set de entrenamiento
  var Xtransposed = transpose(Xunproc);
  
  Xtransposed.forEach((x) => {
    maxList.push(Math.max(...x));
    minList.push(Math.min(...x));
  });
  
  var Xnorm = [];
  Xtransposed.forEach((x) => {
    Xnorm.push(normalize(x));
    // console.log(normalize(x));
  });
  
  Xnorm = transpose(Xnorm);
  const Xtensor = tf.tensor(Xnorm);
  
  // Preparar salida del set de entrenamiento
  
  const array1s = Array(69).fill(1);
  const array0s = Array(69).fill(0);
  const Ytensor = tf.tensor(array1s.concat(array0s));
  
  // console.log(X.print())
  // console.log(Y.print())
  
  const ts = {
    x: Xtensor,
    y: Ytensor
  };

  return ts;
}


/**
 * Función para transponer matrices
 * @param {*} matrix 
 * @returns 
 */
function transpose(matrix) {
  return matrix[0].map((col, i) => matrix.map(row => row[i]));
}

module.exports = {predict};